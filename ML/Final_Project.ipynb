{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMP 551 - Final Project\n",
    "# Track 1, Paper 11\n",
    "\n",
    "# Sebastian Andrade - 260513637\n",
    "# Patrick Beland    - 260688796\n",
    "# Bogdan Dumitru    - 260690446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bogdan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Common Imports\n",
    "import csv\n",
    "import os\n",
    "import operator\n",
    "from random import shuffle\n",
    "from math import log2\n",
    "import math, sys, random, string, re, csv\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Data Processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Quality of life module showing time to completion of cell\n",
    "from tqdm import tqdm\n",
    "\n",
    "#NLTK imports\n",
    "import nltk\n",
    "from nltk.tbl.template import Template\n",
    "from nltk.tag import RegexpTagger, BrillTaggerTrainer\n",
    "from nltk.tag.brill import Pos, Word\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import tnt\n",
    "from nltk import ngrams\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt') #no need to run this line every time\n",
    "\n",
    "#SciPy imports\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import entropy\n",
    "\n",
    "#Scikit learn imports\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "import sklearn.metrics \n",
    "from sklearn.metrics.pairwise import distance_metrics \n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "#Useful base directory path for importing files\n",
    "base_dir = os.path.dirname(os.path.realpath('_file_'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Implementation for Task 1 of Paper 11 - Language Identification\n",
    "\n",
    "## Reads all text files in given directory\n",
    "def process_folder(directory):\n",
    "    data = []\n",
    "    chars = []\n",
    "    filename_order = []\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        with open(os.path.join(directory, filename), encoding='utf-8') as f:\n",
    "                par = []\n",
    "                car = []\n",
    "                content = f.read()\n",
    "                for word in list(content):\n",
    "                    par.append(ord(word))\n",
    "                    car.append(word)\n",
    "                filename_order.append(filename)\n",
    "                chars.append(car)\n",
    "                data.append(par)\n",
    "    return data, filename_order, chars\n",
    "        \n",
    "def write_to_file(file_name, data):\n",
    "    with open(file_name, \"w\", newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=' ')\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "    return \n",
    "\n",
    "## Represents text as tokens of n characters\n",
    "def get_tokens(dataset, n):\n",
    "    total_tokens = {}\n",
    "    for paragraph in tqdm(dataset):\n",
    "        str1 = ' '.join(str(e) for e in paragraph)\n",
    "        tokens = word_tokenize(str1)\n",
    "        grams = ngrams(tokens, n)\n",
    "        for gram in grams:\n",
    "            if gram in total_tokens:\n",
    "                total_tokens[gram] = total_tokens.get(gram) + 1\n",
    "            else:\n",
    "                total_tokens[gram] = 1\n",
    "    return total_tokens       \n",
    "\n",
    "## Generates the frequency bag of words of N-grams\n",
    "def make_tokens(stuff):\n",
    "    total_tokens1 = get_tokens(stuff, 1)\n",
    "    total_tokens2 = get_tokens(stuff, 2)\n",
    "    total_tokens3 = get_tokens(stuff, 3)\n",
    "    total_tokens4 = get_tokens(stuff, 4)\n",
    "    total_tokens5 = get_tokens(stuff, 5)\n",
    "    \n",
    "    sorted_tokens1 = sorted(total_tokens1.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_tokens2 = sorted(total_tokens2.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_tokens3 = sorted(total_tokens3.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_tokens4 = sorted(total_tokens4.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_tokens5 = sorted(total_tokens5.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    sorted_tokens1 = sorted_tokens1[:1000]\n",
    "    sorted_tokens2 = sorted_tokens2[:1000]\n",
    "    sorted_tokens3 = sorted_tokens3[:1000]\n",
    "    sorted_tokens4 = sorted_tokens4[:1000]\n",
    "    sorted_tokens5 = sorted_tokens5[:1000]\n",
    "    return [sorted_tokens1, sorted_tokens2, sorted_tokens3, sorted_tokens4, sorted_tokens5]\n",
    "\n",
    "## Converts vocabuly into a dictionary for convenience\n",
    "def make_dict(vocabs):\n",
    "    list_dix = []\n",
    "    for entry in vocabs:\n",
    "        dick = {}\n",
    "        for index, tup in enumerate(entry):\n",
    "            dick[tup[0]] = index\n",
    "        list_dix.append(dick)\n",
    "    return list_dix\n",
    "\n",
    "def load_vocab(file):\n",
    "    vocab = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            cur_line = line.split()\n",
    "            vocab.append((cur_line[0], cur_line[1], cur_line[2]))\n",
    "\n",
    "## Creates 5000 features representing each text for the whole dataset\n",
    "def featurize(data):\n",
    "    counter = 0\n",
    "    converted_data = []\n",
    "    for paragraph in tqdm(data):\n",
    "        str1 = ' '.join(str(e) for e in paragraph)\n",
    "        if counter % 1000 == 0:\n",
    "            print(str(int(counter / 1000) + 1) + \" out of \" + str(len(data)/1000))\n",
    "        tokens = word_tokenize(str1)\n",
    "        cur_paragraph = np.zeros(5000)\n",
    "        for n in range(5):\n",
    "            grams = ngrams(tokens, n+1)\n",
    "            for gram in grams:\n",
    "                if gram in total_vocab[n]:\n",
    "                    cur_index = n * 1000 + total_vocab[n][gram]\n",
    "                    cur_paragraph[cur_index] += 1\n",
    "        counter += 1\n",
    "        # normalize(cur_paragraph[:,np.newaxis], axis=0).ravel()\n",
    "        #print(cur_paragraph)\n",
    "        #print(\"norm: \")\n",
    "        result =  normalize(cur_paragraph[:,np.newaxis], norm='l1', axis=0).ravel()\n",
    "        \n",
    "        #norm = np.linalg.norm(cur_paragraph)\n",
    "        #print(norm2)\n",
    "        #result = cur_paragraph/ norm2\n",
    "        converted_data.append(result )\n",
    "        #print(\"result: \")\n",
    "        #print(result)\n",
    "    return \n",
    "\n",
    "## Maps string labels to integer\n",
    "def label_to_int(str_labels):\n",
    "    lbls = [] \n",
    "    for lbl in str_labels:\n",
    "        lbls.append(label_list.index(lbl))\n",
    "    return np.array(lbls)\n",
    "\n",
    "\n",
    "## Calculates the skewdivergence of given inputs\n",
    "# alpha is the interpolation parameter\n",
    "#epsilon ensures that there wont be divisions by zero\n",
    "def skewDivergence(all_x, all_y=None, alpha=0.99, epsilon = 0.0000000000000000000001):\n",
    "    result_labels = []\n",
    "    for x in tqdm(all_x):\n",
    "        min_dist = 1000000000\n",
    "        \n",
    "        for label, y in enumerate(all_y):\n",
    "            y =  alpha * y + (1-alpha) * x + epsilon\n",
    "            x = x + epsilon\n",
    "            dist = 0\n",
    "            for idx, yi in enumerate(y):\n",
    "                dist += (x[idx] *(log2(x[idx]) - log2(yi)))\n",
    "            if(dist < min_dist ):\n",
    "                min_dist = dist\n",
    "                lbl = label\n",
    "        result_labels.append(lbl)\n",
    "    return result_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4963/4963 [00:45<00:00, 109.15it/s]\n",
      "100%|██████████| 4963/4963 [00:47<00:00, 104.08it/s]\n",
      "100%|██████████| 4963/4963 [01:11<00:00, 69.41it/s]\n",
      "100%|██████████| 4963/4963 [00:50<00:00, 98.32it/s]\n",
      "100%|██████████| 4963/4963 [00:54<00:00, 91.75it/s]\n",
      "  0%|          | 0/4963 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of 4.963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 998/4963 [00:31<02:04, 31.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 out of 4.963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1999/4963 [00:54<01:21, 36.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 out of 4.963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 2994/4963 [01:14<00:48, 40.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 out of 4.963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 3993/4963 [01:27<00:21, 45.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 out of 4.963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4963/4963 [01:39<00:00, 49.71it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'distance_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-584231b9c17d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[0mtrn_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_to_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrn_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistance_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[0mdictlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'distance_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "#Language Identification Model Implementation\n",
    "\n",
    "#Wikipedia dataset, included in submission\n",
    "directory_wikipedia =  base_dir + '/naacl2010-langid/Wikipedia/'\n",
    "filename_meta_wiki =  base_dir + '/naacl2010-langid/Wikipedia.meta'\n",
    "\n",
    "data_wiki, order_filename, chars_wiki = process_folder(directory_wikipedia)\n",
    "\n",
    "wiki_label = {}\n",
    "with open(filename_meta_wiki, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        a = line.split('\\t')\n",
    "        wiki_label[a[0]]=a[2]\n",
    "        \n",
    "\n",
    "trn = make_tokens(data_wiki)\n",
    "dic_trn = make_dict(trn)\n",
    "total_vocab = dic_trn\n",
    "  \n",
    "wiki_ds_features = featurize(data_wiki)\n",
    "\n",
    "labels = []\n",
    "for fn in order_filename:\n",
    "    labels.append(wiki_label[fn])\n",
    "\n",
    "label_list = (list(set(labels)))\n",
    "    \n",
    "    \n",
    "labeled_data = []\n",
    "for i in range(len(wiki_ds_features)):\n",
    "    labeled_data.append( (wiki_ds_features[i], labels[i]) )\n",
    "    \n",
    "shuffle(labeled_data)\n",
    "\n",
    "factor = 0.8\n",
    "factor_ = 0.1\n",
    "\n",
    "lmt = int(factor*len(labeled_data))\n",
    "lmt_ = int( (factor+factor_) *len(labeled_data))\n",
    "\n",
    "\n",
    "trn_x = []\n",
    "tst_x = []\n",
    "trn_y = []\n",
    "tst_y = []\n",
    "val_x = []\n",
    "val_y = []\n",
    "\n",
    "for idx, entry in enumerate(labeled_data):\n",
    "    if(idx < lmt):\n",
    "        #trn\n",
    "        trn_x.append(entry[0])\n",
    "        trn_y.append(entry[1])\n",
    "    else:\n",
    "        if(idx< lmt_):\n",
    "            tst_x.append(entry[0])\n",
    "            tst_y.append(entry[1])\n",
    "        else:\n",
    "            val_x.append(entry[0])\n",
    "            val_y.append(entry[1])\n",
    "        \n",
    "trn_x_not_sparse = trn_x\n",
    "tst_x_not_sparse = tst_x\n",
    "trn_x = csr_matrix(trn_x)\n",
    "tst_x = csr_matrix(tst_x)\n",
    "\n",
    "tst_y = label_to_int(tst_y)\n",
    "trn_y = label_to_int(trn_y)\n",
    "\n",
    "distances = distance_metrics()\n",
    "\n",
    "dictlist = []\n",
    "for key, value in distances.items():\n",
    "    dictlist.append(key)\n",
    "\n",
    "dictlist.remove('manhattan')\n",
    "dictlist.remove('precomputed')\n",
    "\n",
    "clf = NearestCentroid()\n",
    "clf.fit(trn_x_not_sparse, trn_y)\n",
    "centroids = clf.centroids_\n",
    "\n",
    "pred_y= skewDivergence(tst_x_not_sparse, centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: SkewDivergence\tF1 Score: 0.558739332650623\tAccuracy: 0.8844221105527639\n",
      "Metric: cityblock      \tF1 Score: 0.4694705419201302\tAccuracy: 0.7788944723618091\n",
      "Metric: cosine      \tF1 Score: 0.535226725369209\tAccuracy: 0.7889447236180904\n",
      "Metric: euclidean      \tF1 Score: 0.5345437065378414\tAccuracy: 0.7889447236180904\n",
      "Metric: l2      \tF1 Score: 0.5345437065378414\tAccuracy: 0.7889447236180904\n",
      "Metric: l1      \tF1 Score: 0.4694705419201302\tAccuracy: 0.7788944723618091\n"
     ]
    }
   ],
   "source": [
    "#Language Identification Test Results\n",
    "\n",
    "score = f1_score(tst_y, pred_y, average='macro')  \n",
    "acc = accuracy_score(tst_y, pred_y)\n",
    "print(\"Metric: SkewDivergence\\tF1 Score: \" + str(score) + \"\\tAccuracy: \" + str(acc) )\n",
    "\n",
    "\n",
    "clf_met = []\n",
    "for idx, i in enumerate(dictlist):\n",
    "    clf_met.append(NearestCentroid(metric=i))\n",
    "    clf_met[idx].fit(trn_x, trn_y)\n",
    "    y_pred= clf_met[idx].predict(tst_x)\n",
    "    score = f1_score(tst_y, y_pred, average='macro')  \n",
    "    acc = accuracy_score(tst_y, y_pred)\n",
    "    print(\"Metric: \" + i + \"      \\tF1 Score: \" + str(score) + \"\\tAccuracy: \" + str(acc) )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(file_name):\n",
    "    with open(file_name, encoding='utf8') as f:\n",
    "        for i, n in enumerate(f):\n",
    "            pass\n",
    "    return i+1\n",
    "\n",
    "def generate_data(list_of_files, percent_files_used=1):\n",
    "    tagged_sentences = []\n",
    "    for name in list_of_files:\n",
    "        abs_path = name\n",
    "        print(abs_path)\n",
    "        cur_line = []\n",
    "        num_files = int(file_len(abs_path) * percent_files_used)\n",
    "        with open(abs_path, encoding='utf8') as f:\n",
    "            for index, line in enumerate(f):\n",
    "                if index < num_files:\n",
    "                    line = line.split()\n",
    "                    if not line or line[0] == \"#\":\n",
    "                        if cur_line:\n",
    "                            tagged_sentences.append(cur_line)\n",
    "                            cur_line = []\n",
    "                    else:\n",
    "                        cur_line.append((line[1], line[3]))\n",
    "    return tagged_sentences\n",
    "def give_me_the_money():\n",
    "    minscores = [2]\n",
    "    maxrules = [500]\n",
    "    minacc = [0.99]\n",
    "    cutoffs = [0.1]\n",
    "    for minscore in minscores:\n",
    "        for rule in maxrules:\n",
    "            for acc in minacc:\n",
    "                for cutoff_p in cutoffs:\n",
    "                    cutoff = int(len(tagged_sentences_train) * cutoff_p )\n",
    "                    Template._cleartemplates()\n",
    "                    baseline_tagger = UnigramTagger(train=tagged_sentences_train[:cutoff], backoff=backoff)\n",
    "                    templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]\n",
    "                    tt = BrillTaggerTrainer(baseline_tagger, templates=templates, trace=3)\n",
    "                    tagger1 = tt.train(tagged_sentences_train[cutoff:], max_rules=rule, min_acc=acc, min_score=minscore)\n",
    "\n",
    "                    print(\n",
    "                    'params are: max_rules = ' + str(rule) +\n",
    "                    \" ,min_acc = \" + str(acc) +\n",
    "                    \" ,min score = \" + str(minscore) +\n",
    "                    \" ,cutoff = \" + cutoff_p +\n",
    "                    \" ,number of files trained = \" + str(len(tagged_sentences_train)) +\n",
    "                    \" ,number of files tested = \" + str(len(tagged_sentences_valid))\n",
    "                          )\n",
    "\n",
    "                    print(tagger1.evaluate(tagged_sentences_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\bg-ud-train.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\cs-ud-train-c.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\cs-ud-train-l.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\cs-ud-train-m.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\cs-ud-train-v.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\da-ud-train.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\de-ud-train.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\el-ud-train.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\en-ud-train.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\es-ud-train.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\fa-ud-train.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\fi-ud-train.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\fr-ud-train.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\id-ud-train.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\it-ud-train.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Training_Data\\sv-ud-train.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Test_Data\\bg-ud-test.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Test_Data\\cs-ud-test.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Test_Data\\da-ud-test.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Test_Data\\de-ud-test.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Test_Data\\el-ud-test.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Test_Data\\en-ud-test.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Test_Data\\es-ud-test.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Test_Data\\fa-ud-test.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Test_Data\\fi-ud-test-official.txt\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Test_Data\\fi-ud-test.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Test_Data\\fr-ud-test.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Test_Data\\id-ud-test.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Test_Data\\it-ud-test.conllu\n",
      "D:\\My Doc\\Homework\\2018 Winter\\COMP 551\\Final_Project/ud-treebanks-v1.1/Test_Data\\sv-ud-test.conllu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Baseline Implementation for Task 1 of Paper 11 - Part-of-Speech(POS) Tagging\n",
    "\n",
    "#Define file locations\n",
    "list_of_files_train = glob(base_dir + '/ud-treebanks-v1.1/Training_Data/*')\n",
    "list_of_files_test = glob(base_dir + '/ud-treebanks-v1.1/Test_Data/*')\n",
    "\n",
    "tagged_sentences_train = generate_data(list_of_files_train, percent_files_used=1)\n",
    "tagged_sentences_test = generate_data(list_of_files_test)\n",
    "\n",
    "\n",
    "\n",
    "N = [200]\n",
    "for n in N:\n",
    "    model = tnt.TnT(N = n)\n",
    "    model.train(tagged_sentences_train)\n",
    "    test = model.evaluate(tagged_sentences_test)\n",
    "    sp_un = s.unknown / (s.known + s.unknown)\n",
    "    sp_kn = s.known / (s.known + s.unknown)\n",
    "    print(\"N = \", n)\n",
    "    print(\"Accuracy: \", test)\n",
    "    print('Percentage known:', sp_kn)\n",
    "    print('Percentage unknown:', sp_un)\n",
    "    print('Accuracy over known words:', (sacc / sp_kn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "give_me_the_money()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
